import re


def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if "\\" in text:
        text = bytes(text, "utf-8").decode("unicode_escape")
    text = re.sub(
        r"[\x00-\x1F\x7F]+", " ", text
    )  # –ø–µ—Ä–µ–±–∏—Ä–∞–µ—Ç ASCII —Å–∏–º–≤–æ–ª—ã —Å –∫–æ–¥–∞–º–∏ –æ—Ç 0 –¥–æ 31
    text = (
        text.casefold() if casefold else text
    )  # —Ñ—É–Ω–∫—Ü–∏—è casefold —Å–æ–∑–¥–∞—ë—Ç –∫–æ–ø–∏—é —Å—Ç—Ä–æ–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞–µ—Ç —Å–≤–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    # text = text.replace("\\r\\", " ")
    text = re.sub(r"\s+", " ", text.strip())
    text = text.replace("—ë", "–µ") if yo2e else text
    return text


print(normalize("–ü—Ä–ò–≤–ï—Ç\\n–ú–ò—Ä\\t"))


def tokenize(text: str) -> list[str]:
    allowed_chars = "a-zA-Z–∞-—è—ë–ê-–Ø–Ø–Å0-9- "
    text = re.sub(f"[^{allowed_chars}]", " ", text)
    text = text.strip()
    text = re.sub(r"\s+", " ", text)
    return text.split()


def count_freq(tokens: list[str]) -> dict[str, int]:
    counts = {}
    for tok in tokens:
        counts[tok] = (
            counts.get(tok, 0) + 1
        )  # —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–¥–∞—ë—Ç –∫–ª—é—á—É —Å–ª–æ–≤–∞—Ä—è –∑–Ω–∞—á–µ–Ω–∏–µ, —Å–Ω–∞—á–∞–ª–∞ —Å—á–∏—Ç–∞–µ—Ç –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ, –∞ –ø–æ—Ç–æ–º –¥–æ–±–∞–≤–ª—è–µ—Ç –∫ –Ω–µ–º—É 1 –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ
    return counts


def top_n(freq: dict[str, int], n: int = 2) -> list[tuple[str, int]]:
    words = list(sorted(freq.items(), key=lambda x: (-x[1], x[0])))[:n]
    return words


# normalize
assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞") == "–µ–∂–∏–∫, –µ–ª–∫–∞"
assert normalize("Hello\\r\\nWorld") == "hello world"
assert normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"

# tokenize
assert tokenize("–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä!") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]


# count_freq + top_n
freq = count_freq(["a", "b", "a", "c", "b", "a"])
assert freq == {"a": 3, "b": 2, "c": 1}
assert top_n(freq, 2) == [("a", 3), ("b", 2)]

# —Ç–∞–π-–±—Ä–µ–π–∫ –ø–æ —Å–ª–æ–≤—É –ø—Ä–∏ —Ä–∞–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
freq2 = count_freq(["bb", "aa", "bb", "aa", "cc"])
assert top_n(freq2, 2) == [("aa", 2), ("bb", 2)]
